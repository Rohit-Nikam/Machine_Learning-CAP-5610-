# -*- coding: utf-8 -*-
"""program1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17AJ8YpFH3ONm5tWFOWxvwrxx8dk4y-KX
"""

import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedShuffleSplit

def sort_by_target(mnist):
    reorder_train = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[:60000])]))[:, 1]
    reorder_test = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[60000:])]))[:, 1]
    mnist.data[:60000] = mnist.data[reorder_train]
    mnist.target[:60000] = mnist.target[reorder_train]
    mnist.data[60000:] = mnist.data[reorder_test + 60000]
    mnist.target[60000:] = mnist.target[reorder_test + 60000]

try:
    from sklearn.datasets import fetch_openml
    mnist = fetch_openml('mnist_784', version=1, cache=True)
    mnist.target = mnist.target.astype(np.int8) # fetch_openml() returns targets as strings
    sort_by_target(mnist) # fetch_openml() returns an unsorted dataset
except ImportError:
    from sklearn.datasets import fetch_mldata
    mnist = fetch_mldata('MNIST original')
mnist["data"], mnist["target"]

# %matplotlib inline
some_digit = mnist['data'][36000]
some_digit = some_digit.reshape(28, 28)
plt.imshow(some_digit, cmap = matplotlib.cm.binary,interpolation="nearest")
plt.axis("off")
plt.show()

mnist.data.shape

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
split.get_n_splits(mnist['data'], mnist['target'])

for train_index, test_index in split.split(mnist['data'], mnist['target']):
    print("TRAIN:", len(train_index), "TEST:", len(test_index))
    X_train, X_test = mnist.data[train_index], mnist.data[test_index]
    y_train, y_test = mnist.target[train_index], mnist.target[test_index]

# %matplotlib inline
some_digit = X_train[-1]
some_digit = some_digit.reshape(28, 28)
plt.imshow(some_digit, cmap = matplotlib.cm.binary,interpolation="nearest")
plt.axis("off")
plt.show()

#X_train = X_train    # not to shuffle here...problematic ahead
#y_train = y_train 
print('Shape of training set predictors :',X_train.shape)
print('Shape of training set target :',y_train.shape)

y_train[5]

y_train_9 = (y_train==9)
y_train_9.shape

theta = np.random.randn(1,X_train.shape[1])  ## this is correct
theta.shape

bias = np.ones((1,1))   ## bias  doubtful on shape I thinks it should be 1 x 1
bias.shape



## we are training a number 9 classifier
n_epochs = 100
t0, t1 = 5, 50  # learning schedule hyperparameters
m = len(X_train)
learning_rate = 0.005
batch_size = 500
result_loss = []


def mse_loss(yi,sigma):
  return np.square(sigma - yi).mean()
  
  
def sigmoid(t):
    return 1/(1+np.exp(-t))

def loss_function(theta,bias,xi,yi):   #here xi is transposed
    #print('here')
    z = np.dot(theta,np.transpose(xi)) + bias
    sigma = sigmoid(z)
    diff_sigma = sigmoid(sigma)   #
    print('diff_sigma :',diff_sigma.shape)
    penalty = np.sum(-(np.transpose(yi)-sigma),axis = 1, keepdims =True)/len(xi)
    temp = np.multiply(penalty,diff_sigma*(1-diff_sigma))   #
    print('temp :',z.shape,sigma.shape, temp.shape)         #
    diff_loss = np.dot(temp,xi)     #
    diff_bias = np.sum(temp)
    loss = mse_loss(yi,sigma)
    print('yes :',loss.shape,diff_loss.shape, diff_bias.shape)  
    return loss,diff_loss,diff_bias         #
    
    
for epoch in range(n_epochs):
    shuffled_indices = np.random.permutation(m)       
    X_train = X_train[shuffled_indices]
    y_train_9 = y_train_9[shuffled_indices]
    for i in range(0,m,batch_size):
        random_index = np.random.randint(m-(batch_size+2))
        xi = X_train[random_index:random_index+batch_size]
        yi = y_train_9[random_index:random_index+batch_size]
        print('xi: ',xi.shape)
        print('yi: ',yi.shape)
        loss,diff_loss,diff_bias = loss_function(theta,bias,xi,yi)
        #eta = learning_schedule(epoch * m + i)
        theta = theta - learning_rate * diff_loss
        bias = bias - learning_rate * diff_bias
        #loop+=1
        result_loss.append(loss)
        #print('theta: ',theta.shape)
theta_min = theta 
bias_min = bias
print('success')

theta_min.shape

bias_min

y_test[685]

theta_min.T.shape

X_test[685].T.shape

theta_min.dot(X_test[685])+bias_min

